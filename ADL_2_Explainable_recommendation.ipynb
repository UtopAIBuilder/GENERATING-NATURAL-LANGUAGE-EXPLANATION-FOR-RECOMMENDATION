{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL:2_Explainable_recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeHKeJBU2DjL"
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "import io\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pdb\n",
        "import math"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZTYt9gaGx5k"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    cuda0 = torch.device('cuda:0')\n",
        "else:\n",
        "    cuda0 = torch.device('cpu')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAm2asWdEPk8"
      },
      "source": [
        "reviews_path = '/content/drive/MyDrive/ExplainableRecommedation/reviews.pickle'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVPu7T2dIIiv",
        "outputId": "1ec92bee-e44c-49cb-be99-1624c7f22be6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjj5CgKNKfUq"
      },
      "source": [
        "reviews = pickle.load(open(reviews_path, 'rb'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ctuxyJ7J1M"
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJJpawIgEoI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def load_data(data_path, max_word_num, data_size):\n",
        "    tuple_list = [] # (u,i,rating,reviews)\n",
        "    # collect all users id and items id\n",
        "    doc_list = []\n",
        "    user_set = set()\n",
        "    item_set = set()\n",
        "    feature_set = set()\n",
        "\n",
        "    max_rating = 5\n",
        "    min_rating = 1\n",
        "    count=0\n",
        "    for review in reviews:\n",
        "        count+=1\n",
        "        if count>data_size:\n",
        "            break\n",
        "        user_set.add(review['user'])\n",
        "        item_set.add(review['item'])\n",
        "        rating = review['rating']\n",
        "        if max_rating < rating:\n",
        "            max_rating = rating\n",
        "        if min_rating > rating:\n",
        "            min_rating = rating\n",
        "\n",
        "        if 'text' in review:\n",
        "            doc_list.extend(review['text'].split('\\n')[1].lower().split('.'))\n",
        "        if 'sentence' in review:\n",
        "            quadtruple_list = review['sentence']\n",
        "            for quadtruple in quadtruple_list:\n",
        "                # doc_list.append(quadtruple[2].lower())\n",
        "                feature_set.add(quadtruple[0].lower())\n",
        "\n",
        "    # convert id to array index\n",
        "    user_list = list(user_set)\n",
        "    item_list = list(item_set)\n",
        "    #feature_list = list(feature_set)\n",
        "    user2index = {x: i for i, x in enumerate(user_list)}\n",
        "    item2index = {x: i for i, x in enumerate(item_list)}\n",
        "\n",
        "    word2index, word_list, index2word = get_word2index(doc_list, max_word_num)\n",
        "    return word2index, index2word, word_list, user2index, item2index, user_list, item_list, feature_set\n",
        "\n",
        "\n",
        "def get_word2index(doc_list, max_word_num):\n",
        "    def split_words_by_space(text):\n",
        "        return text.split(' ')\n",
        "\n",
        "    vectorizer = CountVectorizer(max_features=max_word_num, analyzer=split_words_by_space)\n",
        "    vectorizer.fit(doc_list)\n",
        "    word_list = ['<PAD>']+vectorizer.get_feature_names()\n",
        "    word_list.extend(['<UNK>', '<GO>', '<EOS>'])\n",
        "    word2index = {w: i for i, w in enumerate(word_list)}\n",
        "    index2word = {word2index[w]:w for w in word2index.keys()}\n",
        "\n",
        "    return word2index, word_list, index2word"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzWGUCjsyuuP"
      },
      "source": [
        "def format_data(data_size):\n",
        "    tuple_list = []\n",
        "    seq_len_list = []\n",
        "    max_num_of_words = 0\n",
        "    max_num_of_sentences = 0\n",
        "    m_s_rev = None\n",
        "    m_w_rev = None\n",
        "    count=0\n",
        "    for rev in reviews:\n",
        "        count+=1\n",
        "        if count>data_size:\n",
        "            break\n",
        "        if 'sentence' not in rev:\n",
        "            continue\n",
        "        u = user2index[rev['user']]\n",
        "        i = item2index[rev['item']]\n",
        "        r = rev['rating']\n",
        "        sen_indexes = []\n",
        "        sentences = [sen.lower().strip().split(' ') for sen in rev['text'].split('\\n')[1].split('.')]\n",
        "        temp_list = []\n",
        "        #n_sen = min(len(sentences),10)\n",
        "        n_sen = 0\n",
        "        for sen in sentences:\n",
        "            if len(sen)>=2:\n",
        "                n_sen+=1\n",
        "                h = len(sen)\n",
        "                if h>100:\n",
        "                    h = 100\n",
        "                temp_list.append(h+2) # <GO> and <EOS>\n",
        "                f = set(sen[:h]).intersection(feature_set)\n",
        "                w_list = [word2index['<GO>']]+[word2index.get(w, word2index['<UNK>']) for w in sen[:h]]\n",
        "                w_list.append(word2index['<EOS>'])\n",
        "                beta = len(f)/(len(word_list)-1)\n",
        "                w_list.append(beta)\n",
        "                sen_indexes.append(w_list)\n",
        "            if n_sen==3:\n",
        "                break\n",
        "        n_w = max(temp_list)\n",
        "        #n_sen = len(temp_list)\n",
        "        if n_w > max_num_of_words:\n",
        "            max_num_of_words = n_w\n",
        "            m_w_rev = sentences\n",
        "        if n_sen > max_num_of_sentences:\n",
        "            max_num_of_sentences = n_sen\n",
        "            m_s_rev = sentences\n",
        "        seq_len_list.append(temp_list)\n",
        "        tup = (u,i,r,sen_indexes)\n",
        "        tuple_list.append(tup)\n",
        "    return tuple_list, seq_len_list, max_num_of_sentences, max_num_of_words#, m_s_rev, m_w_rev"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APsUAOGu0qK1"
      },
      "source": [
        "max_vocab_len = 10000\n",
        "data_size = 9000\n",
        "word2index, index2word, word_list, user2index, item2index, user_list, item_list, feature_set = load_data(reviews_path, max_vocab_len, data_size)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwP_u2_p28ov",
        "outputId": "7a971d31-196c-436b-c7dc-89cc9b9f19b0"
      },
      "source": [
        "len(word2index)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A397ca4B1B1d"
      },
      "source": [
        "tuple_list, seq_len_list, max_num_of_sentences, max_num_of_words = format_data(data_size) # [u,i,r,sen;beta]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MXZNacRBsCZ",
        "outputId": "c3535863-1ba4-47f1-998b-dd8f7d9f32e9"
      },
      "source": [
        "len(feature_set),max_num_of_sentences, max_num_of_words #, m_s_rev"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 3, 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGXsEVvW1Z7m"
      },
      "source": [
        "rev_data = np.zeros((len(tuple_list),max_num_of_sentences,max_num_of_words), dtype=np.int64) # beta value\n",
        "beta_data = np.zeros((len(tuple_list),max_num_of_sentences), dtype = np.float32)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t21QDRXz0tsw"
      },
      "source": [
        "u = np.zeros(len(tuple_list), dtype=np.int64)\n",
        "i = np.zeros(len(tuple_list), dtype=np.int64)\n",
        "r = np.zeros((len(tuple_list),1), dtype=np.float32)\n",
        "feature_indexes = torch.from_numpy(np.array([word2index.get(w, word2index['<UNK>']) for w in feature_set], dtype=np.int64)).to(cuda0)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irETqJp-uHy7"
      },
      "source": [
        "for idx,tup in enumerate(tuple_list):\n",
        "    #tup = (u,i,r,sen_indexes)\n",
        "    u[idx],i[idx],r[idx],sen_indexes = tup\n",
        "    for k in range(len(sen_indexes)):\n",
        "        rev_data[idx,k,:seq_len_list[idx][k]] = sen_indexes[k][:-1]\n",
        "        beta_data[idx,k] =  sen_indexes[k][-1]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oQYq2VHZUd"
      },
      "source": [
        "tuple_list = None; feature_set = None"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7gpEin85JER",
        "outputId": "438cb129-3dbc-471d-c00b-114167a496c2"
      },
      "source": [
        "np.max(u), len(user2index)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2975, 2976)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OdrcNFtHDJU"
      },
      "source": [
        "Batch_size = 50"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC_PhkEh0sdf",
        "outputId": "c6f57ac3-2d5d-4ab0-b1ea-f6b5b51de4ba"
      },
      "source": [
        "len(word_list)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juOhcOtu4-5Z"
      },
      "source": [
        "dataset = TensorDataset(torch.from_numpy(u).to(cuda0), torch.from_numpy(i).to(cuda0),\n",
        "                        torch.from_numpy(r).to(cuda0), torch.from_numpy(rev_data).to(cuda0), torch.from_numpy(beta_data).to(cuda0))\n",
        "train_data, validation_data = train_test_split(dataset,test_size = .33, random_state=0)\n",
        "train_loader = DataLoader(train_data,batch_size=Batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_data,batch_size=Batch_size, shuffle=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NvdCnp1VKfT"
      },
      "source": [
        "C 0 = φ(W u c 0 u + W v c 0 v + b c 0 )\n",
        "h w\n",
        "n,0 = W n,2 φ(W n,1 (C n ; u; v; o n ) + b n,1 ) + b n,2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8sT4WKX9HK3"
      },
      "source": [
        "Model Building and Shit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOodm_967p9F"
      },
      "source": [
        "class Recommender(nn.Module):\n",
        "    def __init__(self,num_layers, u_dim, i_dim, hidden_dim, word_dim, vocab_len, feature_indexes, max_sentences, max_words, n_user, n_item):\n",
        "        super(Recommender, self).__init__()\n",
        "\n",
        "        # The Rating Module... i/p: u,i; o/p:rating\n",
        "        self.u_embed = nn.Embedding(n_user, u_dim, padding_idx=word2index['<PAD>'])\n",
        "        self.i_embed = nn.Embedding(n_item, i_dim, padding_idx=word2index['<PAD>'])\n",
        "        self.f1 = nn.Linear(in_features=u_dim+i_dim, out_features=hidden_dim)\n",
        "        self.f2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
        "        self.f3 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
        "        self.f4 = nn.Linear(in_features=hidden_dim, out_features=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.feature_indexes = feature_indexes\n",
        "        self.vocab_len = vocab_len\n",
        "\n",
        "        # The NLP Module... i/p: u,i, o/p: set of sentences\n",
        "        self.num_layers = num_layers\n",
        "        self.word = nn.Embedding(vocab_len, word_dim, padding_idx=word2index['<PAD>'])\n",
        "        self.c_0_layer = nn.Linear(u_dim+i_dim, hidden_dim)\n",
        "        self.atten1 = nn.Linear(hidden_dim + word_dim, hidden_dim)\n",
        "        self.atten2 = nn.Linear(hidden_dim,1)\n",
        "        self.initial_hid_gru_word_layer_1 = nn.Linear(hidden_dim + u_dim + i_dim + word_dim, hidden_dim)\n",
        "        self.initial_hid_gru_word_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.Gru_word = nn.GRU(word_dim, hidden_dim, num_layers = num_layers)\n",
        "        self.Gru_context = nn.GRU(hidden_dim, hidden_dim, num_layers = num_layers)\n",
        "        self.max_sentences = max_sentences\n",
        "        self.max_words = max_words\n",
        "        self.final_layer_1 = nn.Linear(hidden_dim, vocab_len//2)\n",
        "        self.final_layer_2 = nn.Linear(vocab_len//2, vocab_len)\n",
        "        self.p_word_embed_1 = nn.Linear(word_dim+u_dim+i_dim, hidden_dim)\n",
        "        self.p_word_embed_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.p_word_embed_3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    \n",
        "    def attention(self,h):\n",
        "        # h = [num_layers, batch_size, hidden_dim]\n",
        "        # feature_indexes = [n_feature]\n",
        "        # pdb.set_trace()\n",
        "        # print(h.shape)\n",
        "        num_layers = h.shape[0]\n",
        "        hid_dim = h.shape[-1]\n",
        "        h = h.view(-1,hid_dim)\n",
        "        # [num_layers*batch_size, hidden_dim]\n",
        "        batch_size = h.shape[0]\n",
        "        src_len = len(feature_indexes)\n",
        "        self.feature_embed = self.word(feature_indexes).repeat(batch_size,1,1) # [num_layers*batch_size, n_features, word_dim]\n",
        "        h = h.unsqueeze(1).repeat(1,src_len,1) # [num_layers*batch_size, n_features, hidden_dim]\n",
        "        # print(h.shape,self.feature_embed.shape)\n",
        "        x = self.atten1(torch.cat((h,self.feature_embed), dim =2))\n",
        "        x = self.relu(x)\n",
        "        # [num_layers*batch_size, n_features, hidden_dim]\n",
        "        x = self.atten2(x).squeeze(2)\n",
        "        return F.softmax(x,dim=1) # [num_layers*batch_size, n_features]\n",
        "\n",
        "    def ratingMod(self):\n",
        "        # u = [batch_size]\n",
        "        # i = [batch_size]\n",
        "        # [batch_size,u_dim]\n",
        "        x = torch.cat((self.latent_u,self.latent_v),dim =1)\n",
        "        x = self.f1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.f2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.f3(x)\n",
        "        x = self.tanh(x)\n",
        "        return self.f4(x)\n",
        "\n",
        "    def getInitialHiddenState(self,C_0):\n",
        "        # C_0 : [num_layers, batch_size, hidden_dim]\n",
        "        # pdb.set_trace()\n",
        "        hid_dim = C_0.shape[2]\n",
        "        atten_score = self.attention(C_0).unsqueeze(1)\n",
        "        # atten_score = [num_layers*batch_size, 1, n_features]\n",
        "        latent_u = self.latent_u.repeat(C_0.shape[0],1)\n",
        "        latent_v = self.latent_v.repeat(C_0.shape[0],1)\n",
        "        # self.feature_embed = [num_layers*batch_size, n_features, word_dim]\n",
        "        o_0 = torch.bmm(atten_score,self.feature_embed).squeeze(1)\n",
        "        # o_0 = [num_layers*batch_size, word_dim]\n",
        "        x = torch.cat((C_0.view(-1,hid_dim),latent_u,latent_v,o_0), dim =1)\n",
        "        # h w n,0 = W n,2 φ(W n,1 (C n ; u; v; o n ) + b n,1 ) + b n,2\n",
        "        x = self.initial_hid_gru_word_layer_1(x)\n",
        "        x = self.relu(x)\n",
        "        hid_dim = x.shape[-1]\n",
        "        # [num_layers*batch_size, hid_dim]\n",
        "        return self.initial_hid_gru_word_layer_2(x).view(self.num_layers,-1,hid_dim)\n",
        "\n",
        "\n",
        "    def forward(self, u,i,rev):\n",
        "        # rev = [batch_size, max_len_sentences, max_len_words]\n",
        "        self.latent_u = self.u_embed(u)\n",
        "        self.latent_v = self.i_embed(i)\n",
        "        ratings = self.ratingMod()\n",
        "\n",
        "        # time for NLP shit....\n",
        "        x = torch.cat((self.latent_u,self.latent_v),dim =1)\n",
        "        C_0 = self.relu(self.c_0_layer(x)).unsqueeze(0).repeat(self.num_layers,1,1)\n",
        "        # [num_layers, batch_size, hidden_dim]\n",
        "        batch_size = self.latent_u.shape[0]\n",
        "        decoder_outputs = self.latent_u.new_zeros((batch_size,self.max_sentences, self.max_words-1, self.vocab_len))\n",
        "        if rev is not None: # training mode\n",
        "            #decoder_outputs = rev.new_zeros((batch_size,self.max_sentence, self.max_words, self.vocab_len))\n",
        "            for i in range(self.max_sentences):\n",
        "                h_0 = self.getInitialHiddenState(C_0)\n",
        "                # [num_layers, batch_size, hid_dim]\n",
        "                for j in range(self.max_words-1): # excluding the <EOS> as input for next word prediction\n",
        "                    w_index = rev[:,i,j]\n",
        "                    w_embed = self.word(w_index)\n",
        "                    # w_embed = self.relu(self.p_word_embed(torch.cat((w_embed,latent_u,latent_v), dim =1)))\n",
        "                    w_embed = self.relu(self.p_word_embed_1(torch.cat((w_embed,self.latent_u,self.latent_v), dim =1)))\n",
        "                    w_embed = self.relu(self.p_word_embed_2(w_embed))\n",
        "                    w_embed = self.p_word_embed_3(w_embed).unsqueeze(0)\n",
        "                    # [1,batch_size, word_dim]\n",
        "                    O_w, h_0 = self.Gru_word(w_embed,h_0)\n",
        "                    # [1, batch, hidden_size] [num_layers, batch, hidden_size]\n",
        "                    x = self.final_layer_1(O_w.squeeze(0))\n",
        "                    x = self.relu(x)\n",
        "                    out = self.final_layer_2(x)\n",
        "                    #x.register_hook(print)\n",
        "                    #x[:,word2index['<UNK>']] += -500 #-math.inf\n",
        "                    #out = F.log_softmax(x, dim =1)\n",
        "                    # [batch_size, vocab_len]\n",
        "                    decoder_outputs[:,i,j,:] = out\n",
        "                O_s, C_0 = self.Gru_context(O_w,C_0)\n",
        "            return decoder_outputs, ratings\n",
        "        else: # testing mode\n",
        "            for i in range(self.max_sentences):\n",
        "                h_0 = self.getInitialHiddenState(C_0)\n",
        "                # [batch_size, hidden_dim]\n",
        "                w_index = torch.full((batch_size,),word2index['<GO>']).to(cuda0)\n",
        "                for j in range(self.max_words-1):\n",
        "                    w_embed = self.word(w_index)\n",
        "                    # w_embed = self.relu(self.p_word_embed(torch.cat((w_embed,latent_u,latent_v), dim =1)))\n",
        "                    w_embed = self.relu(self.p_word_embed_1(torch.cat((w_embed,self.latent_u,self.latent_v), dim =1)))\n",
        "                    w_embed = self.relu(self.p_word_embed_2(w_embed))\n",
        "                    w_embed = self.p_word_embed_3(w_embed).unsqueeze(0)\n",
        "                    # [1,batch_size, word_dim]\n",
        "                    O_w, h_0 = self.Gru_word(w_embed,h_0)\n",
        "                    # [1, batch, hidden_size] [num_layers, batch, hidden_size]\n",
        "                    x = self.final_layer_1(O_w.squeeze(0))\n",
        "                    x = self.relu(x)\n",
        "                    out = self.final_layer_2(x)\n",
        "                    #x[:,word2index['<UNK>']] = -500 #-math.inf\n",
        "                    #out = F.log_softmax(x, dim =1)\n",
        "                    # [batch_size, vocab_len]\n",
        "                    w_index = torch.argmax(out,dim=1)\n",
        "                    decoder_outputs[:,i,j,:] = out\n",
        "                O_s,C_0 = self.Gru_context(O_w,C_0)\n",
        "            return decoder_outputs, ratings"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfeq4-Jrfj3y"
      },
      "source": [
        "u_dim, i_dim, hidden_dim, word_dim = 300,300,300,300\n",
        "num_layers =2\n",
        "vocab_len = len(word_list)\n",
        "model = Recommender(num_layers,u_dim, i_dim, hidden_dim, word_dim, vocab_len, feature_indexes, max_num_of_sentences, max_num_of_words, len(user2index), len(item2index))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US4-MV9tgUli"
      },
      "source": [
        "model.to(cuda0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0W3cA78v1wN"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = .005,weight_decay=.001)\n",
        "NLP_criterion = nn.CrossEntropyLoss(ignore_index=word2index['<PAD>'])\n",
        "Rating_criterion = nn.MSELoss()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ-Z54I2NGso"
      },
      "source": [
        "def training(model, iterator, optimizer, NLP_criterion,Rating_criterion, clip, scaling_factor):\n",
        "    nlp_epoch_loss = 0\n",
        "    rating_epoch_loss = 0\n",
        "    for u,i,r,rev,beta in iterator:\n",
        "        decoder_outputs, ratings = model(u,i,rev) # normailized decoder output\n",
        "        # [batch_size, max_sentence, self.max_words-1, self.vocab_len], [batch_size]\n",
        "        # beta = [batch_size, max_sentence]\n",
        "        # rev = [batch_size, max_sentence, self.max_words]\n",
        "        # r = [batch_size]\n",
        "        # rating = [batch_size]\n",
        "        #pdb.set_trace()\n",
        "        n_classes = decoder_outputs.shape[-1]\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3).repeat(1,1,model.max_words-1,n_classes)\n",
        "        # [batch_size, max_sentence, self.max_words-1]\n",
        "        weighted_decoder_outputs = decoder_outputs#*beta\n",
        "        targ = rev[:,:,1:].reshape(-1)\n",
        "        pred = weighted_decoder_outputs.view(-1,n_classes)\n",
        "        loss1 = NLP_criterion(pred,targ)#*scaling_factor\n",
        "        loss2 = Rating_criterion(ratings,r)\n",
        "        #print(loss1,loss2)\n",
        "        loss = loss1+loss2\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        nlp_epoch_loss+=loss1.item()\n",
        "        rating_epoch_loss+=loss2.item()\n",
        "    return nlp_epoch_loss/len(iterator), rating_epoch_loss/len(iterator)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKkQu_p8NQqe"
      },
      "source": [
        "clip=1\n",
        "scaling_factor = 10**6\n",
        "epochs = 5\n",
        "nlp_loss_list = []\n",
        "rating_loss_list = []\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    nlp_loss, rating_loss = training(model,train_loader, optimizer,\n",
        "                                     NLP_criterion,Rating_criterion, clip, scaling_factor)\n",
        "    end = time.time()\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/ExplainableRecommedation/C_model.pt')\n",
        "    rating_loss_list.append(rating_loss)\n",
        "    nlp_loss_list.append(nlp_loss)\n",
        "    print('epoch number:',epoch+1,'time per epoch(secs):', end-start,\n",
        "          'nlp_loss:',nlp_loss, 'rating_loss:',rating_loss, 'total_loss:',nlp_loss + rating_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw2NPekJf3kr"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ExplainableRecommedation/C_model.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bE9O4Tjj-WN"
      },
      "source": [
        "epoch_loss = 0\n",
        "for u,i,r,rev,beta in val_loader:\n",
        "    decoder_outputs, ratings = model(u,i,None)\n",
        "    loss2 = torch.sqrt(Rating_criterion(ratings,r))\n",
        "    epoch_loss+=loss2\n",
        "print(epoch_loss/len(val_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOtTN1k7ZWbG"
      },
      "source": [
        "x = torch.argmax(decoder_outputs,dim=3)\n",
        "x.shape\n",
        "x = x.to('cpu').numpy()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib896-1D4KKs"
      },
      "source": [
        "i=np.random.randint(0,x.shape[0])\n",
        "for j in range(x.shape[1]):\n",
        "    w_list = x[i,j]\n",
        "    #print(w_list)\n",
        "    w_list = [index2word[w] for w in w_list]\n",
        "    print(' '.join(w_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofB9fmXTxttc"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}